{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "espnet.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ThsrbqCHURN",
        "outputId": "661ae5c3-f648-4ba3-a092-4e9f5a6214b7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 420
        }
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ln -s /content/drive/My\\ Drive /mydrive\n",
        "!ls /mydrive\n",
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "'Colab Notebooks'   Documents\t        speech-recognition\n",
            " data\t\t    envs\t        tilosite.gsite\n",
            " debug\t\t    language-modeling   wandb.key\n",
            "Wed Oct 14 08:07:54 2020       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 455.23.05    Driver Version: 418.67       CUDA Version: 10.1     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   54C    P8    11W /  70W |      0MiB / 15079MiB |      0%      Default |\n",
            "|                               |                      |                 ERR! |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgtrBbvKIMAS",
        "outputId": "c2c1c929-74ef-47aa-8e30-0bc7b0053d05",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "apt-get update && apt-get install -qq bc tree sox libsndfile1\n",
        "rm -rf espnet\n",
        "git clone -b tilos_minimal_librispeech https://github.com/dertilo/espnet.git\n",
        "cd espnet && pip install -e .\n",
        "pip install util@git+https://git@github.com/dertilo/util.git#egg=util\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Get:4 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Hit:5 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Get:6 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:7 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Ign:8 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Ign:9 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:10 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:11 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:12 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [836 B]\n",
            "Get:13 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,681 kB]\n",
            "Get:14 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [39.1 kB]\n",
            "Get:15 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [860 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,150 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.6 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,112 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [231 kB]\n",
            "Ign:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:21 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [334 kB]\n",
            "Get:22 http://security.ubuntu.com/ubuntu bionic-security/multiverse amd64 Packages [15.0 kB]\n",
            "Get:23 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,733 kB]\n",
            "Get:24 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [205 kB]\n",
            "Get:25 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,348 kB]\n",
            "Fetched 11.0 MB in 3s (4,143 kB/s)\n",
            "Reading package lists...\n",
            "Selecting previously unselected package libopencore-amrnb0:amd64.\r\n",
            "(Reading database ... \r(Reading database ... 5%\r(Reading database ... 10%\r(Reading database ... 15%\r(Reading database ... 20%\r(Reading database ... 25%\r(Reading database ... 30%\r(Reading database ... 35%\r(Reading database ... 40%\r(Reading database ... 45%\r(Reading database ... 50%\r(Reading database ... 55%\r(Reading database ... 60%\r(Reading database ... 65%\r(Reading database ... 70%\r(Reading database ... 75%\r(Reading database ... 80%\r(Reading database ... 85%\r(Reading database ... 90%\r(Reading database ... 95%\r(Reading database ... 100%\r(Reading database ... 144617 files and directories currently installed.)\r\n",
            "Preparing to unpack .../0-libopencore-amrnb0_0.1.3-2.1_amd64.deb ...\r\n",
            "Unpacking libopencore-amrnb0:amd64 (0.1.3-2.1) ...\r\n",
            "Selecting previously unselected package libopencore-amrwb0:amd64.\r\n",
            "Preparing to unpack .../1-libopencore-amrwb0_0.1.3-2.1_amd64.deb ...\r\n",
            "Unpacking libopencore-amrwb0:amd64 (0.1.3-2.1) ...\r\n",
            "Selecting previously unselected package libmagic-mgc.\r\n",
            "Preparing to unpack .../2-libmagic-mgc_1%3a5.32-2ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libmagic-mgc (1:5.32-2ubuntu0.4) ...\r\n",
            "Selecting previously unselected package libmagic1:amd64.\r\n",
            "Preparing to unpack .../3-libmagic1_1%3a5.32-2ubuntu0.4_amd64.deb ...\r\n",
            "Unpacking libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\r\n",
            "Selecting previously unselected package bc.\r\n",
            "Preparing to unpack .../4-bc_1.07.1-2_amd64.deb ...\r\n",
            "Unpacking bc (1.07.1-2) ...\r\n",
            "Selecting previously unselected package libsox3:amd64.\r\n",
            "Preparing to unpack .../5-libsox3_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\r\n",
            "Unpacking libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Selecting previously unselected package libsox-fmt-alsa:amd64.\r\n",
            "Preparing to unpack .../6-libsox-fmt-alsa_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\r\n",
            "Unpacking libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Selecting previously unselected package libsox-fmt-base:amd64.\r\n",
            "Preparing to unpack .../7-libsox-fmt-base_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\r\n",
            "Unpacking libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Selecting previously unselected package sox.\r\n",
            "Preparing to unpack .../8-sox_14.4.2-3ubuntu0.18.04.1_amd64.deb ...\r\n",
            "Unpacking sox (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Selecting previously unselected package tree.\r\n",
            "Preparing to unpack .../9-tree_1.7.0-5_amd64.deb ...\r\n",
            "Unpacking tree (1.7.0-5) ...\r\n",
            "Setting up tree (1.7.0-5) ...\r\n",
            "Setting up libmagic-mgc (1:5.32-2ubuntu0.4) ...\r\n",
            "Setting up libmagic1:amd64 (1:5.32-2ubuntu0.4) ...\r\n",
            "Setting up libopencore-amrnb0:amd64 (0.1.3-2.1) ...\r\n",
            "Setting up bc (1.07.1-2) ...\r\n",
            "Setting up libopencore-amrwb0:amd64 (0.1.3-2.1) ...\r\n",
            "Setting up libsox3:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Setting up libsox-fmt-base:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Setting up libsox-fmt-alsa:amd64 (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Setting up sox (14.4.2-3ubuntu0.18.04.1) ...\r\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.2) ...\r\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.6/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\r\n",
            "\r\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\r\n",
            "Processing triggers for mime-support (3.60ubuntu1) ...\r\n",
            "Obtaining file:///content/espnet\n",
            "Requirement already satisfied: setuptools>=38.5.1 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (50.3.0)\n",
            "Collecting configargparse>=1.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/bb/79/3045743bb26ca2e44a1d317c37395462bfed82dbbd38e69a3280b63696ce/ConfigArgParse-1.2.3.tar.gz (42kB)\n",
            "Requirement already satisfied: typeguard>=2.7.0 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (2.7.1)\n",
            "Requirement already satisfied: dataclasses in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (0.7)\n",
            "Collecting humanfriendly\n",
            "  Downloading https://files.pythonhosted.org/packages/8e/2d/2f1b0a780b8c948c06c74c8c80e68ac354da52397ba432a1c5ac1923c3af/humanfriendly-8.2-py2.py3-none-any.whl (86kB)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (1.4.1)\n",
            "Collecting matplotlib==3.1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/da/83/d989ee20c78117c737ab40e0318ea221f1aed4e3f5a40b4f93541b369b93/matplotlib-3.1.0-cp36-cp36m-manylinux1_x86_64.whl (13.1MB)\n",
            "Requirement already satisfied: pillow>=6.1.0 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (7.0.0)\n",
            "Collecting editdistance==0.5.2\n",
            "  Downloading https://files.pythonhosted.org/packages/dc/34/381c67595831fc9daa30c9247cfb8116941fc7380b571a390bbff6da011e/editdistance-0.5.2-cp36-cp36m-manylinux1_x86_64.whl (173kB)\n",
            "Requirement already satisfied: gdown in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (3.6.4)\n",
            "Collecting espnet_model_zoo\n",
            "  Downloading https://files.pythonhosted.org/packages/05/45/7bfd124ba1b0d2859a350d71217cb38ada21bf84e1c96ce1c12a1e5d2501/espnet_model_zoo-0.0.0a16-py3-none-any.whl\n",
            "Collecting ctc-segmentation>=1.0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/7f/43/a9bce952d45bfd85615eecbba5ab69fb2a9106b6b512cd0d3af624eced1b/ctc_segmentation-1.1.0.tar.gz (57kB)\n",
            "Requirement already satisfied: tensorboard>=1.14 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (2.3.0)\n",
            "Collecting tensorboardX>=1.8\n",
            "  Downloading https://files.pythonhosted.org/packages/af/0c/4f41bcd45db376e6fe5c619c01100e9b7531c55791b7244815bac6eac32c/tensorboardX-2.1-py2.py3-none-any.whl (308kB)\n",
            "Collecting librosa>=0.8.0\n",
            "  Downloading https://files.pythonhosted.org/packages/26/4d/c22d8ca74ca2c13cd4ac430fa353954886104321877b65fa871939e78591/librosa-0.8.0.tar.gz (183kB)\n",
            "Requirement already satisfied: resampy in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (0.2.2)\n",
            "Collecting pysptk>=0.1.17\n",
            "  Downloading https://files.pythonhosted.org/packages/7c/25/4ea0932fbf0f1db42934b85011c1c825bcf57055ecde7e511f05e9fb9197/pysptk-0.1.18.tar.gz (419kB)\n",
            "Collecting sentencepiece<0.1.90,>=0.1.82\n",
            "  Downloading https://files.pythonhosted.org/packages/98/2c/8df20f3ac6c22ac224fff307ebc102818206c53fc454ecd37d8ac2060df5/sentencepiece-0.1.86-cp36-cp36m-manylinux1_x86_64.whl (1.0MB)\n",
            "Collecting nltk>=3.4.5\n",
            "  Downloading https://files.pythonhosted.org/packages/92/75/ce35194d8e3022203cca0d2f896dbb88689f9b3fce8e9f9cff942913519d/nltk-3.5.zip (1.4MB)\n",
            "Collecting morfessor\n",
            "  Downloading https://files.pythonhosted.org/packages/39/e6/7afea30be2ee4d29ce9de0fa53acbb033163615f849515c0b1956ad074ee/Morfessor-2.0.6-py3-none-any.whl\n",
            "Collecting PyYAML>=5.1.2\n",
            "  Downloading https://files.pythonhosted.org/packages/64/c2/b80047c7ac2478f9501676c988a5411ed5572f35d1beff9cae07d321512c/PyYAML-5.3.1.tar.gz (269kB)\n",
            "Collecting soundfile>=0.10.2\n",
            "  Downloading https://files.pythonhosted.org/packages/eb/f2/3cbbbf3b96fb9fa91582c438b574cff3f45b29c772f94c400e2c99ef5db9/SoundFile-0.10.3.post1-py2.py3-none-any.whl\n",
            "Collecting h5py==2.9.0\n",
            "  Downloading https://files.pythonhosted.org/packages/30/99/d7d4fbf2d02bb30fb76179911a250074b55b852d34e98dd452a9f394ac06/h5py-2.9.0-cp36-cp36m-manylinux1_x86_64.whl (2.8MB)\n",
            "Collecting kaldiio>=2.15.0\n",
            "  Downloading https://files.pythonhosted.org/packages/a0/f6/a72323a04aa8b6727c319a45d360a4e90dcab2ee6affdd711976610fa50b/kaldiio-2.17.0.tar.gz\n",
            "Requirement already satisfied: inflect>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (2.1.0)\n",
            "Collecting unidecode>=1.0.22\n",
            "  Downloading https://files.pythonhosted.org/packages/d0/42/d9edfed04228bacea2d824904cae367ee9efd05e6cce7ceaaedd0b0ad964/Unidecode-1.1.1-py2.py3-none-any.whl (238kB)\n",
            "Collecting pyworld>=0.2.10\n",
            "  Downloading https://files.pythonhosted.org/packages/5a/91/1b3ebd3840a76e50b3695a9d8515a44303a90c74ae13e474647d984d1e12/pyworld-0.2.11.post0.tar.gz (222kB)\n",
            "Collecting nnmnkwii\n",
            "  Downloading https://files.pythonhosted.org/packages/5b/94/3e6264a708e308e597d7563fd39c7aaec2476c21ed41b093a3cffcc8aa4e/nnmnkwii-0.0.21.tar.gz (1.7MB)\n",
            "Collecting espnet_tts_frontend\n",
            "  Downloading https://files.pythonhosted.org/packages/e0/ab/c1e882c7dcc9241d9e422cb3b4f6554a93d5557d560e8ce918c183a80bd8/espnet_tts_frontend-0.0.3-py3-none-any.whl\n",
            "Collecting museval>=0.2.1\n",
            "  Downloading https://files.pythonhosted.org/packages/54/1a/448486d3619d0e091e2b7160cc5920ff4456e1f1de2b49650fe52e50107e/museval-0.3.1-py2.py3-none-any.whl\n",
            "Collecting pystoi>=0.2.2\n",
            "  Downloading https://files.pythonhosted.org/packages/4c/d8/58a5deefb57e3ca78d6421d2ec536880a29ac0e497b2d1baded6f2153beb/pystoi-0.3.3.tar.gz\n",
            "Collecting nara_wpe>=0.0.5\n",
            "  Downloading https://files.pythonhosted.org/packages/4b/d4/11dddfd5f41017df8eda83cbcafab14ba8bf32d23e7697bf9d2bd343d979/nara_wpe-0.0.7-py3-none-any.whl\n",
            "Collecting torch_complex\n",
            "  Downloading https://files.pythonhosted.org/packages/36/30/cc85a1674c70ef8f3f6c6725bf9dc0e88b727afd0c1ec36cb7f33e4c7e5d/torch_complex-0.2.0-py3-none-any.whl\n",
            "Collecting pytorch_wpe\n",
            "  Downloading https://files.pythonhosted.org/packages/a7/6b/8797da1c34d34afad62d7b4adce0409f416636ec8411d5133854dd31eb09/pytorch_wpe-0.0.0-py3-none-any.whl\n",
            "Collecting mir-eval>=0.6\n",
            "  Downloading https://files.pythonhosted.org/packages/0a/fe/be4f7a59ed71938e21e89f23afe93eea0d39eb3e77f83754a12028cf1a68/mir_eval-0.6.tar.gz (87kB)\n",
            "Requirement already satisfied: fastdtw in /usr/local/lib/python3.6/dist-packages (from espnet==0.9.2) (0.3.4)\n",
            "Collecting torch_optimizer\n",
            "  Downloading https://files.pythonhosted.org/packages/c9/48/f670cf4b47c315861d0547f0c2be579cd801304c86e55008492f1acebd01/torch_optimizer-0.0.1a15-py3-none-any.whl (41kB)\n",
            "Collecting torchaudio==0.6.0\n",
            "  Downloading https://files.pythonhosted.org/packages/96/34/c651430dea231e382ddf2eb5773239bf4885d9528f640a4ef39b12894cb8/torchaudio-0.6.0-cp36-cp36m-manylinux1_x86_64.whl (6.7MB)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy>=1.4.1->espnet==0.9.2) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->espnet==0.9.2) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->espnet==0.9.2) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->espnet==0.9.2) (0.10.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib==3.1.0->espnet==0.9.2) (1.2.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from gdown->espnet==0.9.2) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gdown->espnet==0.9.2) (1.15.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (from gdown->espnet==0.9.2) (4.41.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from espnet_model_zoo->espnet==0.9.2) (1.1.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.6/dist-packages (from ctc-segmentation>=1.0.6->espnet==0.9.2) (0.29.21)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (0.4.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (1.17.2)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (3.12.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (0.10.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (1.32.0)\n",
            "Requirement already satisfied: wheel>=0.26; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (0.35.1)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (1.7.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard>=1.14->espnet==0.9.2) (3.2.2)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->espnet==0.9.2) (2.1.8)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->espnet==0.9.2) (0.22.2.post1)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->espnet==0.9.2) (0.16.0)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->espnet==0.9.2) (4.4.2)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.6/dist-packages (from librosa>=0.8.0->espnet==0.9.2) (0.48.0)\n",
            "Collecting pooch>=1.0\n",
            "  Downloading https://files.pythonhosted.org/packages/ce/11/d7a1dc8173a4085759710e69aae6e070d0d432db84013c7c343e4e522b76/pooch-1.2.0-py3-none-any.whl (47kB)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4.5->espnet==0.9.2) (7.1.2)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.6/dist-packages (from nltk>=3.4.5->espnet==0.9.2) (2019.12.20)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.6/dist-packages (from soundfile>=0.10.2->espnet==0.9.2) (1.14.3)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.6/dist-packages (from nnmnkwii->espnet==0.9.2) (0.0)\n",
            "Collecting bandmat>=0.7\n",
            "  Downloading https://files.pythonhosted.org/packages/94/69/c94a1f690ce9dace7de87f9c95ce297e11e8d0c1314f0280027be2194d0b/bandmat-0.7.tar.gz (364kB)\n",
            "Collecting jaconv\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9e/cf1353fb3e81a177bb52ca59a0ebee425f084b7298039a7965c5414d2d62/jaconv-0.2.4.tar.gz\n",
            "Collecting pypinyin\n",
            "  Downloading https://files.pythonhosted.org/packages/38/c2/2a834bf95f3dfab1e7dbafe4bb88140ac40d0ae35bb737994abfbb54cde9/pypinyin-0.39.1-py2.py3-none-any.whl (780kB)\n",
            "Collecting g2p-en\n",
            "  Downloading https://files.pythonhosted.org/packages/d7/d9/b77dc634a7a0c0c97716ba97dd0a28cbfa6267c96f359c4f27ed71cbd284/g2p_en-2.1.0-py3-none-any.whl (3.1MB)\n",
            "Collecting musdb>=0.3.0\n",
            "  Downloading https://files.pythonhosted.org/packages/57/bd/98ba16482f610bcfa7fcc212175dc0bbf11976e0bc69319b4204b6dc3aec/musdb-0.3.1-py2.py3-none-any.whl\n",
            "Collecting simplejson\n",
            "  Downloading https://files.pythonhosted.org/packages/73/96/1e6b19045375890068d7342cbe280dd64ae73fd90b9735b5efb8d1e044a1/simplejson-3.17.2-cp36-cp36m-manylinux2010_x86_64.whl (127kB)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.6/dist-packages (from museval>=0.2.1->espnet==0.9.2) (2.6.0)\n",
            "Requirement already satisfied: bottleneck in /usr/local/lib/python3.6/dist-packages (from nara_wpe>=0.0.5->espnet==0.9.2) (1.3.2)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from mir-eval>=0.6->espnet==0.9.2) (0.16.0)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from torch_optimizer->espnet==0.9.2) (1.6.0+cu101)\n",
            "Collecting pytorch-ranger>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/0d/70/12256257d861bbc3e176130d25be1de085ce7a9e60594064888a950f2154/pytorch_ranger-0.1.1-py3-none-any.whl\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->espnet==0.9.2) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->espnet==0.9.2) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->espnet==0.9.2) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->gdown->espnet==0.9.2) (3.0.4)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->espnet_model_zoo->espnet==0.9.2) (2018.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->espnet==0.9.2) (1.3.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->espnet==0.9.2) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->espnet==0.9.2) (4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard>=1.14->espnet==0.9.2) (4.1.1)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard>=1.14->espnet==0.9.2) (2.0.0)\n",
            "Requirement already satisfied: llvmlite<0.32.0,>=0.31.0dev0 in /usr/local/lib/python3.6/dist-packages (from numba>=0.43.0->librosa>=0.8.0->espnet==0.9.2) (0.31.0)\n",
            "Collecting appdirs\n",
            "  Downloading https://files.pythonhosted.org/packages/3b/00/2344469e2084fb287c2e0b57b72910309874c3245463acd6cf5e3db69324/appdirs-1.4.4-py2.py3-none-any.whl\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from pooch>=1.0->librosa>=0.8.0->espnet==0.9.2) (20.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.6/dist-packages (from cffi>=1.0->soundfile>=0.10.2->espnet==0.9.2) (2.20)\n",
            "Collecting distance>=0.1.3\n",
            "  Downloading https://files.pythonhosted.org/packages/5c/1a/883e47df323437aefa0d0a92ccfb38895d9416bd0b56262c2e46a47767b8/Distance-0.1.3.tar.gz (180kB)\n",
            "Collecting pyaml\n",
            "  Downloading https://files.pythonhosted.org/packages/15/c4/1310a054d33abc318426a956e7d6df0df76a6ddfa9c66f6310274fb75d42/pyaml-20.4.0-py2.py3-none-any.whl\n",
            "Collecting stempeg>=0.1.7\n",
            "  Downloading https://files.pythonhosted.org/packages/29/ab/6e7362cbff21c25e99cfc3ef116057a7f9ebe6f429a44038eef82de3479d/stempeg-0.1.8-py3-none-any.whl (509kB)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=1.14->espnet==0.9.2) (3.1.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.6/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard>=1.14->espnet==0.9.2) (0.4.8)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard>=1.14->espnet==0.9.2) (3.2.0)\n",
            "Building wheels for collected packages: configargparse, ctc-segmentation, librosa, pysptk, nltk, PyYAML, kaldiio, pyworld, nnmnkwii, pystoi, mir-eval, bandmat, jaconv, distance\n",
            "  Building wheel for configargparse (setup.py): started\n",
            "  Building wheel for configargparse (setup.py): finished with status 'done'\n",
            "  Created wheel for configargparse: filename=ConfigArgParse-1.2.3-cp36-none-any.whl size=19329 sha256=0ee77fa764035c13bbfe768c5396f42b4a7157df3493cdf72d8ffe7a2143e46a\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/d6/53/034032da9498bda2385cd50a51a289e88090b5da2d592b1fdf\n",
            "  Building wheel for ctc-segmentation (setup.py): started\n",
            "  Building wheel for ctc-segmentation (setup.py): finished with status 'done'\n",
            "  Created wheel for ctc-segmentation: filename=ctc_segmentation-1.1.0-cp36-cp36m-linux_x86_64.whl size=92123 sha256=4f209487f26ff869d8b3597181be5f8adb5181fb6ee97440a839cb6ec7a718d0\n",
            "  Stored in directory: /root/.cache/pip/wheels/ea/76/4e/b88ad3a93cd80c0fa62ec267f2f4ebac5873ddea15212c0986\n",
            "  Building wheel for librosa (setup.py): started\n",
            "  Building wheel for librosa (setup.py): finished with status 'done'\n",
            "  Created wheel for librosa: filename=librosa-0.8.0-cp36-none-any.whl size=201376 sha256=28d982f99b080c21ba35fca40f38aeb4eb74a69f30f526867873ed06f11e104b\n",
            "  Stored in directory: /root/.cache/pip/wheels/ee/10/1e/382bb4369e189938d5c02e06d10c651817da8d485bfd1647c9\n",
            "  Building wheel for pysptk (setup.py): started\n",
            "  Building wheel for pysptk (setup.py): finished with status 'done'\n",
            "  Created wheel for pysptk: filename=pysptk-0.1.18-cp36-cp36m-linux_x86_64.whl size=950211 sha256=add17b20d0e2e51a3097c13cfea9fe4f43f7570c469e186e66f03d18e024594a\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/96/d2/a163240019c59504402fab713af259026af81a99dea943404a\n",
            "  Building wheel for nltk (setup.py): started\n",
            "  Building wheel for nltk (setup.py): finished with status 'done'\n",
            "  Created wheel for nltk: filename=nltk-3.5-cp36-none-any.whl size=1434676 sha256=1d836711fac55427631be1876a86dbf82bc45d7dee5af2c468456504469e4a8c\n",
            "  Stored in directory: /root/.cache/pip/wheels/ae/8c/3f/b1fe0ba04555b08b57ab52ab7f86023639a526d8bc8d384306\n",
            "  Building wheel for PyYAML (setup.py): started\n",
            "  Building wheel for PyYAML (setup.py): finished with status 'done'\n",
            "  Created wheel for PyYAML: filename=PyYAML-5.3.1-cp36-cp36m-linux_x86_64.whl size=44619 sha256=4cf65441acfeb0ab94327ad68cbdd4911c8ba7dcbf53e7409abbcd96d43cf391\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/c1/ea/cf5bd31012e735dc1dfea3131a2d5eae7978b251083d6247bd\n",
            "  Building wheel for kaldiio (setup.py): started\n",
            "  Building wheel for kaldiio (setup.py): finished with status 'done'\n",
            "  Created wheel for kaldiio: filename=kaldiio-2.17.0-cp36-none-any.whl size=24457 sha256=1160046db5d73c46e8f3e94ada423277ba61176278233a5a663f83e88186b7ec\n",
            "  Stored in directory: /root/.cache/pip/wheels/5d/ed/f5/0ff9e53aab498215f756c4482b9233a97218f7cdaf043e5ef0\n",
            "  Building wheel for pyworld (setup.py): started\n",
            "  Building wheel for pyworld (setup.py): finished with status 'done'\n",
            "  Created wheel for pyworld: filename=pyworld-0.2.11.post0-cp36-cp36m-linux_x86_64.whl size=608636 sha256=37e442ed5027d848c7e605690aeab3938e5a253e3e117eff31aaf334247d765b\n",
            "  Stored in directory: /root/.cache/pip/wheels/dd/af/e5/28059a621233a9204e9322986b2afddb90976ad5b1c05d76d0\n",
            "  Building wheel for nnmnkwii (setup.py): started\n",
            "  Building wheel for nnmnkwii (setup.py): finished with status 'done'\n",
            "  Created wheel for nnmnkwii: filename=nnmnkwii-0.0.21-cp36-cp36m-linux_x86_64.whl size=1881870 sha256=6672b004879a6b5a142567ed2691d6a067cab0f8b8adaba48a82c912ccda6de9\n",
            "  Stored in directory: /root/.cache/pip/wheels/be/44/02/9f421ab97da563141b42a76b58937f17efb07d548e108d9746\n",
            "  Building wheel for pystoi (setup.py): started\n",
            "  Building wheel for pystoi (setup.py): finished with status 'done'\n",
            "  Created wheel for pystoi: filename=pystoi-0.3.3-py2.py3-none-any.whl size=7782 sha256=2f877708956aa4092457bd8326454a206c7b3e6fe8051423540f9231b65e3ab8\n",
            "  Stored in directory: /root/.cache/pip/wheels/6b/6c/b2/d49af4b7ee4ce275cf2511484b043e09b9cb7ae90c1accb17a\n",
            "  Building wheel for mir-eval (setup.py): started\n",
            "  Building wheel for mir-eval (setup.py): finished with status 'done'\n",
            "  Created wheel for mir-eval: filename=mir_eval-0.6-cp36-none-any.whl size=96516 sha256=2ef0c79f93bf728a7a503f4d4135280fb472c669eb4e1f6e8f72e3a63bfa3c08\n",
            "  Stored in directory: /root/.cache/pip/wheels/49/ce/30/730fa72addf275e49d90683b01b3613048b4be3bf7ff8eb6ec\n",
            "  Building wheel for bandmat (setup.py): started\n",
            "  Building wheel for bandmat (setup.py): finished with status 'done'\n",
            "  Created wheel for bandmat: filename=bandmat-0.7-cp36-cp36m-linux_x86_64.whl size=897998 sha256=07c25ffd1c3284b7bce21eaf9f77d1334cbad316afeadb4a5ecaea2fa0b4bb09\n",
            "  Stored in directory: /root/.cache/pip/wheels/54/1e/28/429fffda48c9c9d2be90a5bea465554ac9f8f40bb370dae42d\n",
            "  Building wheel for jaconv (setup.py): started\n",
            "  Building wheel for jaconv (setup.py): finished with status 'done'\n",
            "  Created wheel for jaconv: filename=jaconv-0.2.4-cp36-none-any.whl size=12284 sha256=2a81d11be07733efe72f62f615e8c40a37fbaa91c5e67271ecffa5e7f2f15e25\n",
            "  Stored in directory: /root/.cache/pip/wheels/e1/46/f7/85a7f89bd3263423c8530dfed16083f9a142cc0fc78c81ff32\n",
            "  Building wheel for distance (setup.py): started\n",
            "  Building wheel for distance (setup.py): finished with status 'done'\n",
            "  Created wheel for distance: filename=Distance-0.1.3-cp36-none-any.whl size=16262 sha256=afb894eb8aeb44f058cb81796d8d615d89edfd8c57ab32daba7bb9f506f3cd7e\n",
            "  Stored in directory: /root/.cache/pip/wheels/d5/aa/e1/dbba9e7b6d397d645d0f12db1c66dbae9c5442b39b001db18e\n",
            "Successfully built configargparse ctc-segmentation librosa pysptk nltk PyYAML kaldiio pyworld nnmnkwii pystoi mir-eval bandmat jaconv distance\n",
            "Installing collected packages: configargparse, humanfriendly, matplotlib, editdistance, espnet-model-zoo, ctc-segmentation, tensorboardX, soundfile, appdirs, pooch, librosa, pysptk, sentencepiece, nltk, morfessor, PyYAML, h5py, kaldiio, unidecode, pyworld, bandmat, nnmnkwii, jaconv, pypinyin, distance, g2p-en, espnet-tts-frontend, pyaml, stempeg, musdb, simplejson, museval, pystoi, nara-wpe, torch-complex, pytorch-wpe, mir-eval, pytorch-ranger, torch-optimizer, torchaudio, espnet\n",
            "  Found existing installation: matplotlib 3.2.2\n",
            "    Uninstalling matplotlib-3.2.2:\n",
            "      Successfully uninstalled matplotlib-3.2.2\n",
            "  Found existing installation: editdistance 0.5.3\n",
            "    Uninstalling editdistance-0.5.3:\n",
            "      Successfully uninstalled editdistance-0.5.3\n",
            "  Found existing installation: librosa 0.6.3\n",
            "    Uninstalling librosa-0.6.3:\n",
            "      Successfully uninstalled librosa-0.6.3\n",
            "  Found existing installation: nltk 3.2.5\n",
            "    Uninstalling nltk-3.2.5:\n",
            "      Successfully uninstalled nltk-3.2.5\n",
            "  Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "  Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "  Running setup.py develop for espnet\n",
            "Successfully installed PyYAML-5.3.1 appdirs-1.4.4 bandmat-0.7 configargparse-1.2.3 ctc-segmentation-1.1.0 distance-0.1.3 editdistance-0.5.2 espnet espnet-model-zoo-0.0.0a16 espnet-tts-frontend-0.0.3 g2p-en-2.1.0 h5py-2.9.0 humanfriendly-8.2 jaconv-0.2.4 kaldiio-2.17.0 librosa-0.8.0 matplotlib-3.1.0 mir-eval-0.6 morfessor-2.0.6 musdb-0.3.1 museval-0.3.1 nara-wpe-0.0.7 nltk-3.5 nnmnkwii-0.0.21 pooch-1.2.0 pyaml-20.4.0 pypinyin-0.39.1 pysptk-0.1.18 pystoi-0.3.3 pytorch-ranger-0.1.1 pytorch-wpe-0.0.0 pyworld-0.2.11.post0 sentencepiece-0.1.86 simplejson-3.17.2 soundfile-0.10.3.post1 stempeg-0.1.8 tensorboardX-2.1 torch-complex-0.2.0 torch-optimizer-0.0.1a15 torchaudio-0.6.0 unidecode-1.1.1\n",
            "Collecting util@ git+https://git@github.com/dertilo/util.git#egg=util\n",
            "  Cloning https://****@github.com/dertilo/util.git to /tmp/pip-install-mpx7gbf6/util\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from util@ git+https://git@github.com/dertilo/util.git#egg=util) (1.18.5)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from util@ git+https://git@github.com/dertilo/util.git#egg=util) (1.4.1)\n",
            "Building wheels for collected packages: util\n",
            "  Building wheel for util (setup.py): started\n",
            "  Building wheel for util (setup.py): finished with status 'done'\n",
            "  Created wheel for util: filename=util-0.1-cp36-none-any.whl size=10975 sha256=06b96e3b7253440d2550bd4634bc87c57b3b4cf33cd82230624f92a74e58a677\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-bpcf8ilf/wheels/95/b5/bf/4b1614c05bcd41e9aa6453ed81261e7d7f3d20e168935b0cfd\n",
            "Successfully built util\n",
            "Installing collected packages: util\n",
            "Successfully installed util-0.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'espnet'...\n",
            "ERROR: tensorflow 2.3.0 has requirement h5py<2.11.0,>=2.10.0, but you'll have h5py 2.9.0 which is incompatible.\n",
            "ERROR: plotnine 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\n",
            "ERROR: mizani 0.6.0 has requirement matplotlib>=3.1.1, but you'll have matplotlib 3.1.0 which is incompatible.\n",
            "ERROR: albumentations 0.1.12 has requirement imgaug<0.2.7,>=0.2.5, but you'll have imgaug 0.2.9 which is incompatible.\n",
            "  Running command git clone -q 'https://****@github.com/dertilo/util.git' /tmp/pip-install-mpx7gbf6/util\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 21.2 ms, sys: 9.83 ms, total: 31 ms\n",
            "Wall time: 2min 4s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfJab6LOJYSo",
        "outputId": "074275d8-5584-44fc-85b4-2a245ef3f2b2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 302
        }
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "ls -alth /mydrive/data\n",
        "TARFILE=dev-clean_preprocessed.tar.gz\n",
        "cp /mydrive/data/$TARFILE ./\n",
        "tar xfz $TARFILE -C ./\n",
        "ls -alth\n",
        "du -sh ./dev-clean_preprocessed/\n",
        "\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.9G\n",
            "-rw------- 1 root root 971M Sep 29 14:09 asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best.zip\n",
            "-rw------- 1 root root  55M Sep 28 16:07 dev-clean_preprocessed.tar.gz\n",
            "-rw------- 1 root root 636M Jun  7 08:35 decisions_185_StGB_no_content.jsonl.gz\n",
            "-rw------- 1 root root 279M Jan  9  2020 BverfG_juris_content.jsonl.gz\n",
            "total 56M\n",
            "drwxr-xr-x  1 root root 4.0K Oct 14 08:10 .\n",
            "-rw-------  1 root root  55M Oct 14 08:10 dev-clean_preprocessed.tar.gz\n",
            "drwxr-xr-x 18 root root 4.0K Oct 14 08:08 espnet\n",
            "drwxr-xr-x  1 root root 4.0K Oct 14 08:07 ..\n",
            "drwx------  5 root root 4.0K Oct 14 08:07 drive\n",
            "drwxr-xr-x  1 root root 4.0K Oct  5 16:31 .config\n",
            "drwxr-xr-x  1 root root 4.0K Oct  5 16:31 sample_data\n",
            "drwxrwxr-x  2 1000 1000 184K Sep 28 16:03 dev-clean_preprocessed\n",
            "62M\t./dev-clean_preprocessed/\n",
            "CPU times: user 4.51 ms, sys: 1.89 ms, total: 6.4 ms\n",
            "Wall time: 2.03 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZIQ5PE9oNXe_",
        "outputId": "4201c291-797c-4948-f492-c67ea532a300",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 894
        }
      },
      "source": [
        "%%bash\n",
        "pip install wandb\n"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting wandb\n",
            "  Downloading https://files.pythonhosted.org/packages/80/14/9a2c792e48e01e55913b9495ce0e8a16297e2bc1cc99e86a848d205c91e7/wandb-0.10.5-py2.py3-none-any.whl (1.7MB)\n",
            "Requirement already satisfied: promise<3,>=2.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.3)\n",
            "Collecting configparser>=3.8.1\n",
            "  Downloading https://files.pythonhosted.org/packages/08/b2/ef713e0e67f6e7ec7d59aea3ee78d05b39c15930057e724cc6d362a8c3bb/configparser-5.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: requests<3,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.23.0)\n",
            "Collecting docker-pycreds>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/f5/e8/f6bd1eee09314e7e6dee49cbe2c5e22314ccdb38db16c9fc72d2fa80d054/docker_pycreds-0.4.0-py2.py3-none-any.whl\n",
            "Requirement already satisfied: python-dateutil>=2.6.1 in /usr/local/lib/python3.6/dist-packages (from wandb) (2.8.1)\n",
            "Collecting watchdog>=0.8.3\n",
            "  Downloading https://files.pythonhosted.org/packages/0e/06/121302598a4fc01aca942d937f4a2c33430b7181137b35758913a8db10ad/watchdog-0.10.3.tar.gz (94kB)\n",
            "Requirement already satisfied: protobuf>=3.12.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (3.12.4)\n",
            "Collecting sentry-sdk>=0.4.0\n",
            "  Downloading https://files.pythonhosted.org/packages/1f/08/5eb320799e3085ccc66ec0fc3360421302803f3b784f74959564dbc6cdc9/sentry_sdk-0.19.0-py2.py3-none-any.whl (120kB)\n",
            "Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (5.4.8)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.6/dist-packages (from wandb) (5.3.1)\n",
            "Collecting subprocess32>=3.5.3\n",
            "  Downloading https://files.pythonhosted.org/packages/32/c8/564be4d12629b912ea431f1a50eb8b3b9d00f1a0b1ceff17f266be190007/subprocess32-3.5.4.tar.gz (97kB)\n",
            "Collecting GitPython>=1.0.0\n",
            "  Downloading https://files.pythonhosted.org/packages/c0/d7/b2b0672e0331567157adf9281f41ee731c412ee518ca5e6552c27fa73c91/GitPython-3.1.9-py3-none-any.whl (159kB)\n",
            "Requirement already satisfied: six>=1.13.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (1.15.0)\n",
            "Collecting shortuuid>=0.5.0\n",
            "  Downloading https://files.pythonhosted.org/packages/25/a6/2ecc1daa6a304e7f1b216f0896b26156b78e7c38e1211e9b798b4716c53d/shortuuid-1.0.1-py3-none-any.whl\n",
            "Requirement already satisfied: Click>=7.0 in /usr/local/lib/python3.6/dist-packages (from wandb) (7.1.2)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2020.6.20)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.0.0->wandb) (2.10)\n",
            "Collecting pathtools>=0.1.1\n",
            "  Downloading https://files.pythonhosted.org/packages/e7/7f/470d6fcdf23f9f3518f6b0b76be9df16dcc8630ad409947f8be2eb0ed13a/pathtools-0.1.2.tar.gz\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.12.0->wandb) (50.3.0)\n",
            "Collecting gitdb<5,>=4.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/48/11/d1800bca0a3bae820b84b7d813ad1eff15a48a64caea9c823fc8c1b119e8/gitdb-4.0.5-py3-none-any.whl (63kB)\n",
            "Collecting smmap<4,>=3.0.1\n",
            "  Downloading https://files.pythonhosted.org/packages/b0/9a/4d409a6234eb940e6a78dfdfc66156e7522262f5f2fecca07dc55915952d/smmap-3.0.4-py2.py3-none-any.whl\n",
            "Building wheels for collected packages: watchdog, subprocess32, pathtools\n",
            "  Building wheel for watchdog (setup.py): started\n",
            "  Building wheel for watchdog (setup.py): finished with status 'done'\n",
            "  Created wheel for watchdog: filename=watchdog-0.10.3-cp36-none-any.whl size=73873 sha256=645ffb0f7ea1e065a806e33ed3b4a6213d00f9bf14a315340e8ca8f18f107056\n",
            "  Stored in directory: /root/.cache/pip/wheels/a8/1d/38/2c19bb311f67cc7b4d07a2ec5ea36ab1a0a0ea50db994a5bc7\n",
            "  Building wheel for subprocess32 (setup.py): started\n",
            "  Building wheel for subprocess32 (setup.py): finished with status 'done'\n",
            "  Created wheel for subprocess32: filename=subprocess32-3.5.4-cp36-none-any.whl size=6489 sha256=ef483908a98eb0508b479ded06e967691a77311973a07b392fbda3a87bd6d5bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/68/39/1a/5e402bdfdf004af1786c8b853fd92f8c4a04f22aad179654d1\n",
            "  Building wheel for pathtools (setup.py): started\n",
            "  Building wheel for pathtools (setup.py): finished with status 'done'\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-cp36-none-any.whl size=8785 sha256=1dcc1e5237056fe5598afa74e50e67ca69010948630a06a68bebe880fecbe21b\n",
            "  Stored in directory: /root/.cache/pip/wheels/0b/04/79/c3b0c3a0266a3cb4376da31e5bfe8bba0c489246968a68e843\n",
            "Successfully built watchdog subprocess32 pathtools\n",
            "Installing collected packages: configparser, docker-pycreds, pathtools, watchdog, sentry-sdk, subprocess32, smmap, gitdb, GitPython, shortuuid, wandb\n",
            "Successfully installed GitPython-3.1.9 configparser-5.0.1 docker-pycreds-0.4.0 gitdb-4.0.5 pathtools-0.1.2 sentry-sdk-0.19.0 shortuuid-1.0.1 smmap-3.0.4 subprocess32-3.5.4 wandb-0.10.5 watchdog-0.10.3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "u_6YyZ-2Gksl",
        "outputId": "5aa3d2a3-bad3-4e5a-e1cc-e18520f335be",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 286
        }
      },
      "source": [
        "%%time\n",
        "%%bash \n",
        "# pip install espnet_model_zoo\n",
        "#model_name=\"Shinji Watanabe/librispeech_asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best\"\n",
        "#espnet_model_zoo_download --unpack true --cachedir \"/content/pretrained\" \"$model_name\"\n",
        "#ls -alth /content/pretrained/653d10049fdc264f694f57b49849343e/\n",
        "#zipfile=pretrained/653d10049fdc264f694f57b49849343e/asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best.zip\n",
        "# cp -r $zipfile  /mydrive/data/\n",
        "ls -alth /mydrive/data\n",
        "ZIPFILE=asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best.zip\n",
        "cp /mydrive/data/$ZIPFILE ./\n",
        "unzip $ZIPFILE -d pretrained_espnet"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 1.9G\n",
            "-rw------- 1 root root 971M Sep 29 14:09 asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best.zip\n",
            "-rw------- 1 root root  55M Sep 28 16:07 dev-clean_preprocessed.tar.gz\n",
            "-rw------- 1 root root 636M Jun  7 08:35 decisions_185_StGB_no_content.jsonl.gz\n",
            "-rw------- 1 root root 279M Jan  9  2020 BverfG_juris_content.jsonl.gz\n",
            "Archive:  asr_train_asr_transformer_e18_raw_bpe_sp_valid.acc.best.zip\n",
            " extracting: pretrained_espnet/meta.yaml  \n",
            " extracting: pretrained_espnet/exp/asr_train_asr_transformer_e18_raw_bpe_sp/config.yaml  \n",
            " extracting: pretrained_espnet/exp/lm_train_lm_adam_bpe/config.yaml  \n",
            " extracting: pretrained_espnet/exp/asr_train_asr_transformer_e18_raw_bpe_sp/54epoch.pth  \n",
            " extracting: pretrained_espnet/exp/lm_train_lm_adam_bpe/17epoch.pth  \n",
            " extracting: pretrained_espnet/exp/asr_train_asr_transformer_e18_raw_bpe_sp/RESULTS.md  \n",
            " extracting: pretrained_espnet/exp/asr_stats_raw_sp/train/feats_stats.npz  \n",
            " extracting: pretrained_espnet/data/token_list/bpe_unigram5000/bpe.model  \n",
            "CPU times: user 5.43 ms, sys: 5.38 ms, total: 10.8 ms\n",
            "Wall time: 47.4 s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7ev_F-pFKatN",
        "outputId": "1f98b9ca-8f2a-4ee8-bc0a-c0b8c7d8f508",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "%%bash\n",
        "export WANDB_API_KEY=$(head -n 1 /mydrive/wandb.key | cut -d '=' -f 2 )\n",
        "export WANDB_NAME=\"debug\"\n",
        "export WANDB_NOTES=\"just trying to track GPU usage\"\n",
        "export WANDB_PROJECT=espnet-asr\n",
        "python -m wandb init --project $WANDB_PROJECT\n",
        "\n",
        "rm -rf /tmp/espnet_output # start from zero, use if train-data changes somehow\n",
        "#rm -rf /tmp/espnet_output/train_logs\n",
        "python /mydrive/speech-recognition/espnet_main.py \\\n",
        "    --train_path \"dev-clean_preprocessed\" \\\n",
        "    --eval_path \"dev-clean_preprocessed\" \\\n",
        "    --pretrained_base \"pretrained_espnet\" \\\n",
        "    --num_gpus 1 \\\n",
        "    --batch_bins 3200000\n",
        "    #  --eval_limit 40 \\"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Namespace(batch_bins=3200000, config_yml=None, eval_limit=None, eval_path='dev-clean_preprocessed', is_distributed=False, num_encoder_blocks=1, num_gpus=1, num_workers=1, output_path='/tmp/espnet_output', pretrained_base='pretrained_espnet', train_limit=1, train_path='dev-clean_preprocessed', vocab_size=500)\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]     /root/nltk_data...\n",
            "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n",
            "[nltk_data] Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/cmudict.zip.\n",
            "could not save program above cwd: /mydrive/speech-recognition/espnet_main.py\n",
            "wandb: Currently logged in as: dertilo (use `wandb login --relogin` to force relogin)\n",
            "wandb: Tracking run with wandb version 0.10.5\n",
            "wandb: Syncing run debug\n",
            "wandb: ⭐️ View project at https://wandb.ai/dertilo/espnet-asr\n",
            "wandb: 🚀 View run at https://wandb.ai/dertilo/espnet-asr/runs/15xvidiw\n",
            "wandb: Run data is saved locally in wandb/run-20201014_082026-15xvidiw\n",
            "wandb: Run `wandb off` to turn off syncing.\n",
            "/usr/bin/python3 /mydrive/speech-recognition/espnet_main.py --train_path dev-clean_preprocessed --eval_path dev-clean_preprocessed --pretrained_base pretrained_espnet --num_gpus 1 --batch_bins 3200000\n",
            "[a4f6042d69d2] 2020-10-14 08:20:28,509 (asr:281) INFO: Vocabulary size: 5000\n",
            "[a4f6042d69d2] 2020-10-14 08:20:30,412 (abs_task:1059) INFO: pytorch.version=1.6.0+cu101, cuda.available=True, cudnn.version=7603, cudnn.benchmark=False, cudnn.deterministic=True\n",
            "[a4f6042d69d2] 2020-10-14 08:20:30,417 (abs_task:1060) INFO: Model structure:\n",
            "ESPnetASRModel(\n",
            "  (frontend): DefaultFrontend(\n",
            "    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n",
            "    (frontend): Frontend()\n",
            "    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n",
            "  )\n",
            "  (specaug): SpecAug(\n",
            "    (time_warp): TimeWarp(window=5, mode=bicubic)\n",
            "    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n",
            "    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n",
            "  )\n",
            "  (normalize): GlobalMVN(stats_file=pretrained_espnet/exp/asr_stats_raw_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed): Conv2dSubsampling6(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): Linear(in_features=6144, out_features=512, bias=True)\n",
            "        (1): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (encoders): MultiSequential(\n",
            "      (0): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (6): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (7): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (8): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (9): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (11): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (12): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (13): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (14): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (15): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (16): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (17): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed): Sequential(\n",
            "      (0): Embedding(5000, 512)\n",
            "      (1): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n",
            "    (decoders): MultiSequential(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ctc): CTC(\n",
            "    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n",
            "    (ctc_loss): CTCLoss()\n",
            "  )\n",
            "  (criterion_att): LabelSmoothingLoss(\n",
            "    (criterion): KLDivLoss()\n",
            "  )\n",
            ")\n",
            "\n",
            "Model summary:\n",
            "    Class Name: ESPnetASRModel\n",
            "    Number of parameters: 99.36 M\n",
            "    Size: 397.46 MB\n",
            "    Type: torch.float32\n",
            "[a4f6042d69d2] 2020-10-14 08:20:30,417 (abs_task:1063) INFO: Optimizer:\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0\n",
            "    lr: 0.0\n",
            "    weight_decay: 0\n",
            ")\n",
            "[a4f6042d69d2] 2020-10-14 08:20:30,418 (abs_task:1064) INFO: Scheduler: WarmupLR(warmup_steps=25000)\n",
            "[a4f6042d69d2] 2020-10-14 08:20:30,419 (abs_task:1074) INFO: Saving the configuration in /tmp/espnet_output/stats/config.yaml\n",
            "[a4f6042d69d2] 2020-10-14 08:20:58,558 (collect_stats:110) INFO: Niter: 100\n",
            "/usr/bin/python3 /mydrive/speech-recognition/espnet_main.py --train_path dev-clean_preprocessed --eval_path dev-clean_preprocessed --pretrained_base pretrained_espnet --num_gpus 1 --batch_bins 3200000\n",
            "[a4f6042d69d2] 2020-10-14 08:21:08,193 (asr:281) INFO: Vocabulary size: 5000\n",
            "[a4f6042d69d2] 2020-10-14 08:21:19,851 (abs_task:1059) INFO: pytorch.version=1.6.0+cu101, cuda.available=True, cudnn.version=7603, cudnn.benchmark=False, cudnn.deterministic=True\n",
            "[a4f6042d69d2] 2020-10-14 08:21:19,856 (abs_task:1060) INFO: Model structure:\n",
            "ESPnetASRModel(\n",
            "  (frontend): DefaultFrontend(\n",
            "    (stft): Stft(n_fft=512, win_length=512, hop_length=128, center=True, normalized=False, onesided=True)\n",
            "    (frontend): Frontend()\n",
            "    (logmel): LogMel(sr=16000, n_fft=512, n_mels=80, fmin=0, fmax=8000.0, htk=False)\n",
            "  )\n",
            "  (specaug): SpecAug(\n",
            "    (time_warp): TimeWarp(window=5, mode=bicubic)\n",
            "    (freq_mask): MaskAlongAxis(mask_width_range=[0, 30], num_mask=2, axis=freq)\n",
            "    (time_mask): MaskAlongAxis(mask_width_range=[0, 40], num_mask=2, axis=time)\n",
            "  )\n",
            "  (normalize): GlobalMVN(stats_file=pretrained_espnet/exp/asr_stats_raw_sp/train/feats_stats.npz, norm_means=True, norm_vars=True)\n",
            "  (encoder): TransformerEncoder(\n",
            "    (embed): Conv2dSubsampling6(\n",
            "      (conv): Sequential(\n",
            "        (0): Conv2d(1, 512, kernel_size=(3, 3), stride=(2, 2))\n",
            "        (1): ReLU()\n",
            "        (2): Conv2d(512, 512, kernel_size=(5, 5), stride=(3, 3))\n",
            "        (3): ReLU()\n",
            "      )\n",
            "      (out): Sequential(\n",
            "        (0): Linear(in_features=6144, out_features=512, bias=True)\n",
            "        (1): PositionalEncoding(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (encoders): MultiSequential(\n",
            "      (0): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (6): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (7): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (8): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (9): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (10): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (11): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (12): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (13): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (14): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (15): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (16): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (17): EncoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "  )\n",
            "  (decoder): TransformerDecoder(\n",
            "    (embed): Sequential(\n",
            "      (0): Embedding(5000, 512)\n",
            "      (1): PositionalEncoding(\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "    (after_norm): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "    (output_layer): Linear(in_features=512, out_features=5000, bias=True)\n",
            "    (decoders): MultiSequential(\n",
            "      (0): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (1): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (2): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (3): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (4): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "      (5): DecoderLayer(\n",
            "        (self_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (src_attn): MultiHeadedAttention(\n",
            "          (linear_q): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_k): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_v): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (linear_out): Linear(in_features=512, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "        (feed_forward): PositionwiseFeedForward(\n",
            "          (w_1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (w_2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (activation): ReLU()\n",
            "        )\n",
            "        (norm1): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm2): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (norm3): LayerNorm((512,), eps=1e-12, elementwise_affine=True)\n",
            "        (dropout): Dropout(p=0.1, inplace=False)\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (ctc): CTC(\n",
            "    (ctc_lo): Linear(in_features=512, out_features=5000, bias=True)\n",
            "    (ctc_loss): CTCLoss()\n",
            "  )\n",
            "  (criterion_att): LabelSmoothingLoss(\n",
            "    (criterion): KLDivLoss()\n",
            "  )\n",
            ")\n",
            "\n",
            "Model summary:\n",
            "    Class Name: ESPnetASRModel\n",
            "    Number of parameters: 99.36 M\n",
            "    Size: 397.46 MB\n",
            "    Type: torch.float32\n",
            "[a4f6042d69d2] 2020-10-14 08:21:19,857 (abs_task:1063) INFO: Optimizer:\n",
            "Adam (\n",
            "Parameter Group 0\n",
            "    amsgrad: False\n",
            "    betas: (0.9, 0.999)\n",
            "    eps: 1e-08\n",
            "    initial_lr: 0.0\n",
            "    lr: 0.0\n",
            "    weight_decay: 0\n",
            ")\n",
            "[a4f6042d69d2] 2020-10-14 08:21:19,857 (abs_task:1064) INFO: Scheduler: WarmupLR(warmup_steps=25000)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:19,858 (abs_task:1074) INFO: Saving the configuration in /tmp/espnet_output/train_logs/config.yaml\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,405 (abs_task:1414) INFO: [train] dataset:\n",
            "ESPnetDataset(\n",
            "  speech: {\"path\": \"/tmp/espnet_output/manifests/train/wav.scp\", \"type\": \"sound\"}\n",
            "  text: {\"path\": \"/tmp/espnet_output/manifests/train/text\", \"type\": \"text\"}\n",
            "  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f3cf4d28ac8>)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,405 (abs_task:1415) INFO: [train] Batch sampler: NumElementsBatchSampler(N-batch=1, batch_bins=3200000, sort_in_batch=descending, sort_batch=descending)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,405 (abs_task:1417) INFO: [train] mini-batch sizes summary: N-batch=1, mean=1.0, min=1, max=1\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,466 (abs_task:1414) INFO: [valid] dataset:\n",
            "ESPnetDataset(\n",
            "  speech: {\"path\": \"/tmp/espnet_output/manifests/valid/wav.scp\", \"type\": \"sound\"}\n",
            "  text: {\"path\": \"/tmp/espnet_output/manifests/valid/text\", \"type\": \"text\"}\n",
            "  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f3cf4db67f0>)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,466 (abs_task:1415) INFO: [valid] Batch sampler: NumElementsBatchSampler(N-batch=97, batch_bins=3200000, sort_in_batch=descending, sort_batch=descending)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,466 (abs_task:1417) INFO: [valid] mini-batch sizes summary: N-batch=97, mean=27.9, min=3, max=89\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,495 (abs_task:1414) INFO: [plot_att] dataset:\n",
            "ESPnetDataset(\n",
            "  speech: {\"path\": \"/tmp/espnet_output/manifests/valid/wav.scp\", \"type\": \"sound\"}\n",
            "  text: {\"path\": \"/tmp/espnet_output/manifests/valid/text\", \"type\": \"text\"}\n",
            "  preprocess: <espnet2.train.preprocessor.CommonPreprocessor object at 0x7f3cf4d24ba8>)\n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,495 (abs_task:1415) INFO: [plot_att] Batch sampler: UnsortedBatchSampler(N-batch=2703, batch_size=1, key_file=/tmp/espnet_output/stats/valid/speech_shape, \n",
            "[a4f6042d69d2] 2020-10-14 08:21:20,496 (abs_task:1417) INFO: [plot_att] mini-batch sizes summary: N-batch=3, mean=1.0, min=1, max=1\n",
            "2020-10-14 08:21:20.675979: I tensorflow/stream_executor/platform/default/dso_loader.cc:48] Successfully opened dynamic library libcudart.so.10.1\n",
            "[a4f6042d69d2] 2020-10-14 08:21:21,904 (trainer:196) INFO: 1/1epoch started\n",
            "[a4f6042d69d2] 2020-10-14 08:23:20,678 (trainer:243) INFO: 1epoch results: [train] iter_time=0.223, forward_time=0.241, loss=30.241, loss_att=22.298, loss_ctc=48.773, acc=0.860, backward_time=0.078, optim_step_time=0.093, lr_0=0.000e+00, train_time=0.853, time=4.81 seconds, total_count=1, [valid] loss=4.995, loss_att=4.164, loss_ctc=6.933, acc=0.969, cer=0.020, wer=0.405, cer_ctc=0.024, time=45.16 seconds, total_count=97, [att_plot] time=1 minute and 8.78 seconds, total_count=0\n",
            "[a4f6042d69d2] 2020-10-14 08:23:29,931 (trainer:287) INFO: The best model has been updated: valid.acc\n",
            "[a4f6042d69d2] 2020-10-14 08:23:29,932 (trainer:322) INFO: The training was finished at 1 epochs \n",
            "wandb: Waiting for W&B process to finish, PID 2150\n",
            "wandb: Program ended successfully.\n",
            "wandb: - 0.00MB of 0.00MB uploaded (0.00MB deduped)\rwandb: \\ 0.06MB of 0.06MB uploaded (0.00MB deduped)\rwandb:                                                                                \n",
            "wandb: Find user logs for this run at: wandb/run-20201014_082026-15xvidiw/logs/debug.log\n",
            "wandb: Find internal logs for this run at: wandb/run-20201014_082026-15xvidiw/logs/debug-internal.log\n",
            "wandb: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "wandb: \n",
            "wandb: Synced debug: https://wandb.ai/dertilo/espnet-asr/runs/15xvidiw\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "CPU times: user 14.2 ms, sys: 3.89 ms, total: 18.1 ms\n",
            "Wall time: 3min 17s\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-e1-yxY1UmA",
        "outputId": "42376e32-797e-48da-d44a-8d79aeb8bafa",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 134
        }
      },
      "source": [
        "%%bash\n",
        "ls -alth /tmp/espnet_output"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "total 24K\n",
            "drwxrwxrwt 1 root root 4.0K Oct 14 08:23 ..\n",
            "drwxr-xr-x 5 root root 4.0K Oct 14 08:23 train_logs\n",
            "drwxr-xr-x 5 root root 4.0K Oct 14 08:21 .\n",
            "drwxr-xr-x 4 root root 4.0K Oct 14 08:20 stats\n",
            "drwxr-xr-x 4 root root 4.0K Oct 14 08:20 manifests\n",
            "-rw-r--r-- 1 root root 1.5K Oct 14 08:20 config.yml\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}